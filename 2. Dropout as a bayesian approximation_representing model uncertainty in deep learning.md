# Dropout as a bayesian approximation_representing model uncertainty in deep learning



- 박태준, 정몽주
- 공간통계연구실
- 2022년 8월




## Bayesian Neural Networks


![img](./img/7-image1.png)

- **Bayesian neural networks (BNNs, Bayesian NNs)** place a prior distribution over a neural network’s weights.
    $$p(\boldsymbol \omega)$$
- Bayesian neural networks offer a probabilistic interpretation of deep learning model.
    - **Model uncertainty**

## Dropout


![img](./img/7-image2.png)

- **Dropout** is used in many models in deep learning as a way to avoid over-fitting.
- The key idea is to randomly drop units (along with their connections) from the neural network during training.

## Topic
### Dropout as a Bayesian Approximation

- A neural network with dropout applied before every weight layer is mathematically equivalent to an approximation to a well known Bayesian model : the Gaussian process (GP).
- We develop tools for representing model uncertainty of existing dropout NNs.




# Introduction

### Model Uncertainty

- **Out of distribution test data:**
    - Given several pictures of dog breeds as training data
    - What should happen if a user uploads a photo of a cat and asks the website to decide on a dog breed?

![img](./img/7-image3.png)


- Softmax function converts a vector of K real numbers into a probability distribution of K possible outcomes.
- The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.




![img](./img/7-figure1.png)
Figure: **A sketch of softmax input and output for an idealised binary classification problem.** Ignoring function uncertainty, point $x^\*$ is classified as class 1 with probability 1.


- Standard deep learning tools for regression and classification do not capture model uncertainty.

- Representing model uncertainty is of crucial importance.
- With model confidence at hand we can treat uncertain inputs and special cases explicitly.
    - For example, in the case of classification, a model might return a result with high uncertainty.
    - In this case, we might decide to pass the input to a human for classification.


# Background

### Notation

- $\mathbf x$: Vectors.
- $\mathbf X$: Matrices.
- $x$: scalar quantities.
- $\mathbf x_i$: Entire rows or columns.
- $x_{ij}$: Specific elements.

- $\mathbf W_1: Q\times K, ~\mathbf W_2: K\times D$: Variables.
- $\mathbf w_q, ~\mathbf w_k$: Corresponding lower case indices to refer to specific rows / columns for the first variable.
- $\mathbf w_k, ~\mathbf w_d$: For the second variable.

- $w_{1,qk}$: The element at row $q$ column $k$ of the variable $\mathbf W_1.$
- $w_{2,kd}$: The element at row $k$ column $d$ of the variable $\mathbf W_2.$

## Dropout

- Review the dropout NN model for the case of a single hidden layer NN.
- The generalization to multiple layers is straightforward.

- $\mathbf W_1 ~(Q\times K)$: The weight matrix connecting the first layer to the hidden layer.
- $\mathbf W_2 ~(K\times D)$: The weight matrix connecting connecting the hidden layer to the output layer.
- $\mathbf b$: The biases, $K$ dimensional vector.
- $\sigma(\cdot)$: Activation function, some element-wise non-linearity.

- **A standard NN model would output**: $$\hat{\mathbf y} = \sigma(\mathbf x \mathbf W_1 + \mathbf b) \mathbf W_2$$ given some input $\mathbf x.$

    - Note that we omit the outer-most bias term as this is equivalent to centring the output.

- Dropout is applied by sampling two binary vectors $\mathbf z_1, \mathbf z_2$ of dimensions $Q$ and $K$ respectively, with

    $$\mathbf z_{1,q}  \sim {\rm Bernoulli}(p_1) \text{for} q = 1, \cdots, Q,$$
    $$\mathbf z_{2,k}  \sim {\rm Bernoulli}(p_2) \text{for} k = 1, \cdots, K.$$
- Given an input $\mathbf x,$ $1 - p_1$ proportion of the elements of the input are set to zero: $$\mathbf x \circ \mathbf z_1.$$  
- **The dropout model’s output**: $$\hat{\mathbf{y}} = \sigma\Big( \mathbf x (\mathbf z_1 \mathbf W_1) + \mathbf b \Big)(\mathbf z_2 \mathbf W_2).$$
    - (We will write $\z_1$ when we mean ${\rm diag}(\mathbf z_1)$.)
    
