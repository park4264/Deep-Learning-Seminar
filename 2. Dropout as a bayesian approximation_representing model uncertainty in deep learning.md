# Dropout as a bayesian approximation_representing model uncertainty in deep learning



- 박태준, 정몽주
- 공간통계연구실
- 2022년 8월




## Bayesian Neural Networks


![img](./img/7-image1.png)

- **Bayesian neural networks (BNNs, Bayesian NNs)** place a prior distribution over a neural network’s weights.
    $$p(\boldsymbol \omega)$$
- Bayesian neural networks offer a probabilistic interpretation of deep learning model.
    - **Model uncertainty**

## Dropout


![img](./img/7-image2.png)

- **Dropout** is used in many models in deep learning as a way to avoid over-fitting.
- The key idea is to randomly drop units (along with their connections) from the neural network during training.

## Topic
### Dropout as a Bayesian Approximation

- A neural network with dropout applied before every weight layer is mathematically equivalent to an approximation to a well known Bayesian model : the Gaussian process (GP).
- We develop tools for representing model uncertainty of existing dropout NNs.




# Introduction

## Model Uncertainty

- **Out of distribution test data:**
    - Given several pictures of dog breeds as training data
    - What should happen if a user uploads a photo of a cat and asks the website to decide on a dog breed?

![img](./img/7-image3.png)


- Softmax function converts a vector of K real numbers into a probability distribution of K possible outcomes.
- The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.




![img](./img/7-figure1.png)
Figure: **A sketch of softmax input and output for an idealised binary classification problem.** Ignoring function uncertainty, point $x^\*$ is classified as class 1 with probability 1.


- Standard deep learning tools for regression and classification do not capture model uncertainty.

- Representing model uncertainty is of crucial importance.
- With model confidence at hand we can treat uncertain inputs and special cases explicitly.
    - For example, in the case of classification, a model might return a result with high uncertainty.
    - In this case, we might decide to pass the input to a human for classification.


# Background




