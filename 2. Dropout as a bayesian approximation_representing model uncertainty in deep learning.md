# Dropout as a bayesian approximation_representing model uncertainty in deep learning



- 박태준, 정몽주
- 공간통계연구실
- 2022년 8월




## Bayesian Neural Networks


![img](./img/7-image1.png)

- **Bayesian neural networks (BNNs, Bayesian NNs)** place a prior distribution over a neural network’s weights.
    $$p(\boldsymbol \omega)$$
- Bayesian neural networks offer a probabilistic interpretation of deep learning model.
    - **Model uncertainty**

## Dropout


![img](./img/7-image2.png)

- **Dropout** is used in many models in deep learning as a way to avoid over-fitting.
- The key idea is to randomly drop units (along with their connections) from the neural network during training.

## Topic
### Dropout as a Bayesian Approximation

- A neural network with dropout applied before every weight layer is mathematically equivalent to an approximation to a well known Bayesian model : the Gaussian process (GP).
- We develop tools for representing model uncertainty of existing dropout NNs.




# Introduction

### Model Uncertainty

- **Out of distribution test data:**
    - Given several pictures of dog breeds as training data
    - What should happen if a user uploads a photo of a cat and asks the website to decide on a dog breed?

![img](./img/7-image3.png)


- Softmax function converts a vector of K real numbers into a probability distribution of K possible outcomes.
- The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.




![img](./img/7-figure1.png)
Figure: **A sketch of softmax input and output for an idealised binary classification problem.** Ignoring function uncertainty, point $x^\*$ is classified as class 1 with probability 1.


- Standard deep learning tools for regression and classification do not capture model uncertainty.

- Representing model uncertainty is of crucial importance.
- With model confidence at hand we can treat uncertain inputs and special cases explicitly.
    - For example, in the case of classification, a model might return a result with high uncertainty.
    - In this case, we might decide to pass the input to a human for classification.


# Background

### Notation

- $\mathbf x$: Vectors.
- $\mathbf X$: Matrices.
- $x$: scalar quantities.
- $\mathbf x_i$: Entire rows or columns.
- $x_{ij}$: Specific elements.

- $\mathbf W_1: Q\times K, ~\mathbf W_2: K\times D$: Variables.
- $\mathbf w_q, ~\mathbf w_k$: Corresponding lower case indices to refer to specific rows / columns for the first variable.
- $\mathbf w_k, ~\mathbf w_d$: For the second variable.

- $w_{1,qk}$: The element at row $q$ column $k$ of the variable $\mathbf W_1.$
- $w_{2,kd}$: The element at row $k$ column $d$ of the variable $\mathbf W_2.$

## Dropout

- Review the dropout NN model for the case of a single hidden layer NN.
- The generalization to multiple layers is straightforward.

- $\mathbf W_1 ~(Q\times K)$: The weight matrix connecting the first layer to the hidden layer.
- $\mathbf W_2 ~(K\times D)$: The weight matrix connecting connecting the hidden layer to the output layer.
- $\mathbf b$: The biases, $K$ dimensional vector.
- $\sigma(\cdot)$: Activation function, some element-wise non-linearity.

- **A standard NN model would output**: $$\hat{\mathbf y} = \sigma(\mathbf x \mathbf W_1 + \mathbf b) \mathbf W_2$$ given some input $\mathbf x.$

    - Note that we omit the outer-most bias term as this is equivalent to centring the output.

- Dropout is applied by sampling two binary vectors $\mathbf z_1, \mathbf z_2$ of dimensions $Q$ and $K$ respectively, with

    $$\mathbf z_{1,q}  \sim {\rm Bernoulli}(p_1) \text{for} q = 1, \cdots, Q,$$
    $$\mathbf z_{2,k}  \sim {\rm Bernoulli}(p_2) \text{for} k = 1, \cdots, K.$$
- Given an input $\mathbf x,$ $1 - p_1$ proportion of the elements of the input are set to zero: $$\mathbf x \circ \mathbf z_1.$$  
- **The dropout model’s output**: $$\hat{\mathbf{y}} = \sigma\Big( \mathbf x (\mathbf z_1 \mathbf W_1) + \mathbf b \Big)(\mathbf z_2 \mathbf W_2).$$
    - (We will write $\mathbf z_1$ when we mean ${\rm diag}(\mathbf z_1)$.)
    
- $\lbrace \mathbf x_1, \cdots, \mathbf x_N \rbrace$ : $N$ observed inputs.
- $\lbrace \mathbf y_1, \cdots, \mathbf y_N \rbrace$ : Corresponding observed outputs.
- $\lbrace \hat{ \mathbf y}_1, \cdots ,  \hat{\mathbf y}_N \rbrace$ : The outputs of the model.
- We often use $L_2$ regularization for parameters $\lbrace \mathbf W_1, \mathbf W_2, \mathbf b \rbrace$ weighted by some weight decays $\lambda_i$.
- **Minimization objective (cost)** for regression:
$$\mathcal L_{\rm dropout} := \dfrac{1}{2N} \sum^N_{n = 1} \Vert \mathbf{y}_n - \hat{\mathbf{y}}_n \Vert^2_2  + \lambda_1 \Vert \mathbf{W}_1 \Vert^2_2 + \lambda_2 \Vert \mathbf{W}_2 \Vert^2_2 + \lambda_3 \Vert \mathbf{W}_3 \Vert_2^2$$








## Gaussian Processes

- We use a **Gaussian process (GP)** to describe a distribution over functions.
- The Gaussian process offers desirable properties such as uncertainty estimates over the function values.


> **Definition(Gaussian Process)** A random process $X(t)$ is a \textbf{Gaussian process} if $\forall k \in \mathbb N,$ $\forall t_1, \cdots, t_k \in \mathcal T,$ a random vector formed by $X(t_1), \cdots, X(t_k)$ is jointly Gaussian.


- A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
- The joint density is completely specified by
    - **Mean function**: $m(t) = \mathbb E[X(t)],$ where $m(\cdot)$ is known as a mean function.
    - **Covariance function**: $k(t,s) = \text{cov}(X(t), X(s)),$ where $k(\cdot, \cdot)$ is known as a covariance function.
- **Notation**: $X(t) \sim \mathcal{GP}(m(t), k(t,s))$



- $\lbrace \mathbf x_1, \cdots, \mathbf x_N \rbrace, \lbrace \mathbf y_1, \cdots, \mathbf y_N \rbrace$: Training dataset.
- $\mathbf X \in\mathbb R^{N \times Q}, \mathbf Y \in\mathbb R^{N\times D}$: Inputs and outputs.
- We would like to estimate a function $$\mathbf y = \mathbf f (\mathbf x)$$ that is likely to have generated our observation.
- Following the Bayesian approach, we would put some **prior distribution** over the space of functions $p(\mathbf f)$.
- We then look for the **posterior distribution** over the space of functions given our dataset  $$p(\mathbf f | \mathbf X, \mathbf Y) \propto p(\mathbf Y | \mathbf X, \mathbf f)p(\mathbf f).$$
- In our case the random variables represent the value of the function $\mathbf f(\mathbf x)$ at location $\mathbf x.$


> **Remark (Describe a distribution over functions)**
> - A Gaussian process is completely specified by its mean function and covariance function.
> - Define mean function and the covariance function of $\mathbf f(\mathbf x)$ as 
$$m(\mathbf x) = \mathbb E[\mathbf f(\mathbf x)]$$
$$k(\mathbb x, \mathbb x') = \mathbb E[(\mathbf f(\mathbf x) - m(\mathbf x)) (\mathbf f(\mathbf x') - m(\mathbf x'))^\top ]$$
> - Write the Gaussian process as $$\mathbf f(\mathbf x) \sim \mathcal{GP}(m(\mathbf x), k(\mathbf x, \mathbf x')).$$

- In practice, we place a joint Gaussian distribution over all function values 
$$\mathbf F | \mathbf X \sim \mathcal N (\mathbf 0, \mathbf K(\mathbf X, \mathbf X))$$
- To model the data we have to choose a covariance function $\mathbf K (\mathbf X_1, \mathbf X_2)$ for the Gaussian distribution.

## Variational Inference

- we could condition the model on a finite set of random variables $\omega$.
- Goal of a Bayesian neural network is to find a posterior distribution: $$p(\omega | \mathbf X, \mathbf Y) \propto p(\mathbf Y | \mathbf X, \omega) p(\omega)$$ where $\mathbf X$ and $\mathbf Y$ are the input and output training data and $\omega$ is a set of parameters of our interest.


- Once we have $p(\omega | \mathbf X, \mathbf Y)$, the output $\mathbf y^\*\in \mathbb R^D$ at unseen $\mathbf x^\* \in \mathbb R^Q$ is predicted as: $$p(\mathbf y^\* | \mathbf x^\*,\mathbf X,\mathbf Y) = \int p(\mathbf y^\* | \mathbf x^*\, \omega) p(\omega | \mathbf X, \mathbf Y) d\omega$$ called **the posterior predictive distribution** for a new $\mathbf x^\*.$




- In practice, none of them is tractable
- **Variational inference** is often used to handle this issue.
- The distribution $p(\omega | \mathbf X, \mathbf Y)$ cannot usually be evaluated analytically.
- Instead we define an approximating variational distribution $q(\omega)$, whose structure is easy to evaluate.


$$p(\mathbf y^* | \mathbf x^\*,\mathbf X,\mathbf Y) = \int p(\mathbf y^\* | \mathbf x^\*, \omega) p(\omega | \mathbf X, \mathbf Y) d\omega$$

$$\Downarrow$$

$$p(\mathbf y^\* | \mathbf x^\*,\mathbf X,\mathbf Y)  \approx \int p(\mathbf y^\* | \mathbf x^\*, \omega) \textcolor{red}{q(\omega)} d\omega$$


